{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#WITHOUT CUDA\n"
      ],
      "metadata": {
        "id": "kgOD1QYlkq0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import string\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "unk = '<UNK>'\n",
        "# Consult the PyTorch documentation for information on the functions used below:\n",
        "# https://pytorch.org/docs/stable/torch.html\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, h):  # Add relevant parameters\n",
        "        super(RNN, self).__init__()\n",
        "        self.h = h\n",
        "        self.numOfLayer = 1\n",
        "        self.rnn = nn.RNN(input_dim, h, self.numOfLayer, nonlinearity='tanh')\n",
        "        self.W = nn.Linear(h, 5)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "        self.loss = nn.NLLLoss()\n",
        "\n",
        "    def compute_Loss(self, predicted_vector, gold_label):\n",
        "        return self.loss(predicted_vector, gold_label)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # obtain hidden layer representation\n",
        "        # inputs shape: (seq_len, batch_size, input_dim)\n",
        "        _, hidden = self.rnn(inputs)  # hidden shape: (num_layers, batch_size, hidden_size)\n",
        "\n",
        "        # obtain output layer representations\n",
        "        # We take the last hidden state and pass it through the linear layer\n",
        "        output = self.W(hidden[-1])  # shape: (batch_size, 5)\n",
        "\n",
        "        # obtain probability distribution using log softmax\n",
        "        predicted_vector = self.softmax(output)  # shape: (batch_size, 5)\n",
        "\n",
        "        return predicted_vector\n",
        "\n",
        "\n",
        "def load_data(train_data, val_data, test_data):\n",
        "  # Load training data\n",
        "  with open(train_data) as training_f:\n",
        "      training = json.load(training_f)\n",
        "\n",
        "  # Load validation data\n",
        "  with open(val_data) as valid_f:\n",
        "      validation = json.load(valid_f)\n",
        "\n",
        "  # Load test data\n",
        "  with open(test_data) as test_f:\n",
        "      testing = json.load(test_f)\n",
        "\n",
        "  tra = []\n",
        "  val = []\n",
        "  test = []\n",
        "\n",
        "  # Process training data\n",
        "  for elt in training:\n",
        "      tra.append((elt[\"text\"].split(), int(elt[\"stars\"]-1)))\n",
        "\n",
        "  # Process validation data\n",
        "  for elt in validation:\n",
        "      val.append((elt[\"text\"].split(), int(elt[\"stars\"]-1)))\n",
        "\n",
        "  # Process test data\n",
        "  for elt in testing:\n",
        "      test.append((elt[\"text\"].split(), int(elt[\"stars\"]-1)))\n",
        "\n",
        "  return tra, val, test\n",
        "\n"
      ],
      "metadata": {
        "id": "PviS8JcyYpXa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameter combinations\n",
        "hidden_dims = [16, 64, 256]\n",
        "epochs_list = [5]#[5, 10, 30]\n",
        "\n",
        "# Create a directory for results if it doesn't exist\n",
        "!mkdir -p results\n",
        "\n",
        "# Function to train and evaluate model\n",
        "def train_and_evaluate(hidden_dim, num_epochs, train_data, valid_data, test_data, word_embedding):\n",
        "    model = RNN(50, hidden_dim)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    training_accuracies = []\n",
        "    validation_accuracies = []\n",
        "    training_losses = []\n",
        "\n",
        "    stopping_condition = False\n",
        "    epoch = 0\n",
        "    last_train_accuracy = 0\n",
        "    last_validation_accuracy = 0\n",
        "\n",
        "    max_epochs = num_epochs\n",
        "\n",
        "    while not stopping_condition and epoch < max_epochs:\n",
        "            # Training Phase\n",
        "            random.shuffle(train_data)\n",
        "            model.train()\n",
        "            # You will need further code to operationalize training, ffnn.py may be helpful\n",
        "            print(\"Training started for epoch {}\".format(epoch + 1))\n",
        "            train_data = train_data\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            minibatch_size = 16\n",
        "            N = len(train_data)\n",
        "\n",
        "            loss_total = 0\n",
        "            loss_count = 0\n",
        "            for minibatch_index in tqdm(range(N // minibatch_size)):\n",
        "                optimizer.zero_grad()\n",
        "                loss = None\n",
        "                for example_index in range(minibatch_size):\n",
        "                    idx = minibatch_index * minibatch_size + example_index\n",
        "                    if idx >= len(train_data):\n",
        "                      continue\n",
        "\n",
        "                    input_words, gold_label = train_data[minibatch_index * minibatch_size + example_index]\n",
        "                    input_words = \" \".join(input_words)\n",
        "\n",
        "                    # Remove punctuation\n",
        "                    input_words = input_words.translate(input_words.maketrans(\"\", \"\", string.punctuation)).split()\n",
        "\n",
        "                    # Look up word embedding dictionary\n",
        "                    vectors = [word_embedding[i.lower()] if i.lower() in word_embedding.keys() else word_embedding['unk'] for i in input_words ]\n",
        "\n",
        "                    # Transform the input into required shape\n",
        "                    vectors = torch.tensor(vectors).view(len(vectors), 1, -1)\n",
        "                    output = model(vectors)\n",
        "\n",
        "                    # Get loss\n",
        "                    example_loss = model.compute_Loss(output.view(1,-1), torch.tensor([gold_label]))\n",
        "\n",
        "                    # Get predicted label\n",
        "                    predicted_label = torch.argmax(output)\n",
        "\n",
        "                    correct += int(predicted_label == gold_label)\n",
        "                    # print(predicted_label, gold_label)\n",
        "                    total += 1\n",
        "                    if loss is None:\n",
        "                        loss = example_loss\n",
        "                    else:\n",
        "                        loss += example_loss\n",
        "\n",
        "                loss = loss / minibatch_size\n",
        "                loss_total += loss.data\n",
        "                loss_count += 1\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            epoch_loss = loss_total/loss_count if loss_count > 0 else 0\n",
        "            training_loss = float(epoch_loss)\n",
        "            training_accuracy = correct/total\n",
        "\n",
        "            print(loss_total/loss_count)\n",
        "            print(\"Training completed for epoch {}\".format(epoch + 1))\n",
        "            print(\"Training accuracy for epoch {}: {}\".format(epoch + 1, correct / total))\n",
        "\n",
        "            #VALIDATION PHASE\n",
        "\n",
        "            model.eval()\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            random.shuffle(valid_data)\n",
        "            print(\"Validation started for epoch {}\".format(epoch + 1))\n",
        "            valid_data = valid_data\n",
        "\n",
        "            with torch.no_grad():  # Add this to prevent gradient computation during validation\n",
        "                for input_words, gold_label in tqdm(valid_data):\n",
        "                    input_words = \" \".join(input_words)\n",
        "                    input_words = input_words.translate(input_words.maketrans(\"\", \"\", string.punctuation)).split()\n",
        "                    vectors = [word_embedding[i.lower()] if i.lower() in word_embedding.keys() else word_embedding['unk'] for i\n",
        "                            in input_words]\n",
        "\n",
        "                    vectors = torch.tensor(vectors).view(len(vectors), 1, -1)\n",
        "                    output = model(vectors)\n",
        "                    predicted_label = torch.argmax(output)\n",
        "                    correct += int(predicted_label == gold_label)\n",
        "                    total += 1\n",
        "                    # print(predicted_label, gold_label)\n",
        "            validation_accuracy = correct/total\n",
        "            print(\"Validation completed for epoch {}\".format(epoch + 1))\n",
        "            print(\"Validation accuracy for epoch {}: {}\".format(epoch + 1, correct / total))\n",
        "\n",
        "            # Store metrics\n",
        "            training_accuracies.append(training_accuracy)\n",
        "            validation_accuracies.append(validation_accuracy)\n",
        "            training_losses.append(training_loss)\n",
        "\n",
        "            if validation_accuracy < last_validation_accuracy and training_accuracy > last_train_accuracy:\n",
        "                stopping_condition=True\n",
        "                print(\"Training done to avoid overfitting!\")\n",
        "                print(\"Best validation accuracy is:\", last_validation_accuracy)\n",
        "            else:\n",
        "                last_validation_accuracy = validation_accuracy\n",
        "                last_train_accuracy = training_accuracy\n",
        "\n",
        "            epoch += 1\n",
        "\n",
        "    print(\"========== Training completed ==========\")\n",
        "\n",
        "    return training_accuracies, validation_accuracies, training_losses, test_accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "NiSP8QifY7a0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Load data\n",
        "    print(\"========== Loading data ==========\")\n",
        "    train_data, valid_data, test_data = load_data(\n",
        "        '/content/drive/MyDrive/NLPA2/training.json',\n",
        "        '/content/drive/MyDrive/NLPA2/validation.json',\n",
        "        '/content/drive/MyDrive/NLPA2/test.json'\n",
        "    )\n",
        "\n",
        "    # Load word embeddings\n",
        "    word_embedding = pickle.load(open('/content/drive/MyDrive/NLPA2/word_embedding.pkl', 'rb'))\n",
        "\n",
        "    # Run all combinations\n",
        "    all_results = []\n",
        "\n",
        "    for hidden_dim in hidden_dims:\n",
        "        for num_epochs in epochs_list:\n",
        "            print(f\"\\n========== Training model with hidden_dim={hidden_dim}, epochs={num_epochs} ==========\")\n",
        "\n",
        "            # Train and evaluate model\n",
        "            train_accs, val_accs, train_losses, test_acc = train_and_evaluate(\n",
        "                hidden_dim, num_epochs, train_data, valid_data, test_data, word_embedding\n",
        "            )\n",
        "\n",
        "            # Create results DataFrame\n",
        "            results_df = pd.DataFrame({\n",
        "                'epoch': range(1, len(train_accs) + 1),\n",
        "                'training_accuracy': train_accs,\n",
        "                'validation_accuracy': val_accs,\n",
        "                'training_loss': train_losses,\n",
        "                'test_accuracy': [test_acc] * len(train_accs),\n",
        "                'hidden_dim': [hidden_dim] * len(train_accs),\n",
        "                'max_epochs': [num_epochs] * len(train_accs)\n",
        "            })\n",
        "\n",
        "            # Save individual result\n",
        "            filename = f\"results/rnn_{num_epochs}_{hidden_dim}.csv\"\n",
        "            results_df.to_csv(filename, index=False)\n",
        "            print(f\"Results saved to {filename}\")\n",
        "\n",
        "            # Store summary for combined results\n",
        "            all_results.append({\n",
        "                'hidden_dim': hidden_dim,\n",
        "                'epochs': num_epochs,\n",
        "                'final_train_acc': train_accs[-1],\n",
        "                'final_val_acc': val_accs[-1],\n",
        "                'final_test_acc': test_acc,\n",
        "                'best_val_acc': max(val_accs),\n",
        "                'total_epochs_run': len(train_accs)\n",
        "            })\n",
        "\n",
        "    # Create and save combined results\n",
        "    combined_results = pd.DataFrame(all_results)\n",
        "    combined_results.to_csv('results/combined_results.csv', index=False)\n",
        "    print(\"\\nCombined results saved to results/combined_results.csv\")\n",
        "\n",
        "    # Display combined results\n",
        "    print(\"\\nSummary of all experiments:\")\n",
        "    print(combined_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "m4GM942MZpJl",
        "outputId": "bf6a2c68-3027-4a44-a6cd-ceba406c133f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========== Loading data ==========\n",
            "\n",
            "========== Training model with hidden_dim=16, epochs=5 ==========\n",
            "Training started for epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1000 [00:00<?, ?it/s]<ipython-input-4-0ba796136de9>:56: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  vectors = torch.tensor(vectors).view(len(vectors), 1, -1)\n",
            "  6%|▌         | 61/1000 [00:09<02:32,  6.17it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-0c9f9d457f02>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m# Train and evaluate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             train_accs, val_accs, train_losses, test_acc = train_and_evaluate(\n\u001b[0m\u001b[1;32m     23\u001b[0m                 \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             )\n",
            "\u001b[0;32m<ipython-input-4-0ba796136de9>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(hidden_dim, num_epochs, train_data, valid_data, test_data, word_embedding)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mloss_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mloss_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mminibatch_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1189\u001b[0m                     \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcur_t\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlast_print_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mdt\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmininterval\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcur_t\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmin_start_t\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlast_print_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m                         \u001b[0mlast_print_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_print_n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m                         \u001b[0mlast_print_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_print_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1240\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ema_dn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ema_dt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1242\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlock_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic_miniters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m                     \u001b[0;31m# If no `miniters` was specified, adjust automatically to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mrefresh\u001b[0;34m(self, nolock, lock_args)\u001b[0m\n\u001b[1;32m   1345\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1347\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1348\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnolock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(self, msg, pos)\u001b[0m\n\u001b[1;32m   1493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1495\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mprint_status\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mprint_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0mlen_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisp_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m             \u001b[0mfp_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m             \u001b[0mlast_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mfp_write\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfp_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0mfp_flush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0mlast_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/utils.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                 \u001b[0;31m# and give a timeout to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m                     \u001b[0;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                     \u001b[0;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F-WDhb9BZ0Ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQsoK4dFjYda",
        "outputId": "1547dba3-fb83-4c68-c290-2addcf48db0e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ku2I6dhza7Uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oveabg77a7Si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-_wOWwZoa7Po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oWS3NKwGa7Mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JCkoIqmCa7J3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z_gfOswXkm2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CUDA ENABLED TRAINING"
      ],
      "metadata": {
        "id": "uWIDFGrkkno-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import string\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "# Check if CUDA is available and set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "unk = '<UNK>'\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, h):\n",
        "        super(RNN, self).__init__()\n",
        "        self.h = h\n",
        "        self.numOfLayer = 1\n",
        "        self.rnn = nn.RNN(input_dim, h, self.numOfLayer, nonlinearity='tanh', batch_first=False)\n",
        "        self.W = nn.Linear(h, 5)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "        self.loss = nn.NLLLoss()\n",
        "\n",
        "        # Move model to GPU\n",
        "        self.to(device)\n",
        "\n",
        "    def compute_Loss(self, predicted_vector, gold_label):\n",
        "        return self.loss(predicted_vector, gold_label.to(device))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        _, hidden = self.rnn(inputs)\n",
        "        output = self.W(hidden[-1])\n",
        "        predicted_vector = self.softmax(output)\n",
        "        return predicted_vector\n",
        "\n",
        "def load_data(train_data, val_data, test_data):\n",
        "    # Load and process data (same as before)\n",
        "    with open(train_data) as training_f:\n",
        "        training = json.load(training_f)\n",
        "    with open(val_data) as valid_f:\n",
        "        validation = json.load(valid_f)\n",
        "    with open(test_data) as test_f:\n",
        "        testing = json.load(test_f)\n",
        "\n",
        "    tra = [(elt[\"text\"].split(), int(elt[\"stars\"]-1)) for elt in training]\n",
        "    val = [(elt[\"text\"].split(), int(elt[\"stars\"]-1)) for elt in validation]\n",
        "    test = [(elt[\"text\"].split(), int(elt[\"stars\"]-1)) for elt in testing]\n",
        "\n",
        "    return tra, val, test\n",
        "\n",
        "def process_batch(batch_data, word_embedding):\n",
        "    # Process a batch of data in parallel\n",
        "    processed_inputs = []\n",
        "    labels = []\n",
        "\n",
        "    for input_words, gold_label in batch_data:\n",
        "        input_words = \" \".join(input_words)\n",
        "        input_words = input_words.translate(str.maketrans(\"\", \"\", string.punctuation)).split()\n",
        "        vectors = [word_embedding[i.lower()] if i.lower() in word_embedding else word_embedding['unk']\n",
        "                  for i in input_words]\n",
        "        processed_inputs.append(vectors)\n",
        "        labels.append(gold_label)\n",
        "\n",
        "    # Pad sequences in the batch to same length\n",
        "    max_len = max(len(seq) for seq in processed_inputs)\n",
        "    padded_inputs = [seq + [word_embedding['unk']] * (max_len - len(seq)) for seq in processed_inputs]\n",
        "\n",
        "    # Convert to tensors and move to GPU\n",
        "    inputs_tensor = torch.tensor(padded_inputs, dtype=torch.float32).to(device)\n",
        "    labels_tensor = torch.tensor(labels, dtype=torch.long).to(device)\n",
        "\n",
        "    # Reshape for RNN input (seq_len, batch_size, input_dim)\n",
        "    inputs_tensor = inputs_tensor.permute(1, 0, 2)\n",
        "\n",
        "    return inputs_tensor, labels_tensor\n",
        "\n",
        "def train_and_evaluate(hidden_dim, num_epochs, train_data, valid_data, test_data, word_embedding):\n",
        "    model = RNN(50, hidden_dim)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    training_accuracies = []\n",
        "    validation_accuracies = []\n",
        "    training_losses = []\n",
        "\n",
        "    stopping_condition = False\n",
        "    epoch = 0\n",
        "    last_train_accuracy = 0\n",
        "    last_validation_accuracy = 0\n",
        "    minibatch_size = 32  # Increased batch size for GPU\n",
        "\n",
        "    while not stopping_condition and epoch < num_epochs:\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        random.shuffle(train_data)\n",
        "        print(f\"Training started for epoch {epoch + 1}\")\n",
        "\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        loss_total = 0\n",
        "        loss_count = 0\n",
        "\n",
        "        # Process data in batches\n",
        "        for i in tqdm(range(0, len(train_data), minibatch_size)):\n",
        "            batch_data = train_data[i:i + minibatch_size]\n",
        "            vectors, labels = process_batch(batch_data, word_embedding)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(vectors)\n",
        "            loss = model.compute_Loss(output, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            predicted_labels = torch.argmax(output, dim=1)\n",
        "            correct += (predicted_labels == labels).sum().item()\n",
        "            total += len(labels)\n",
        "\n",
        "            loss_total += loss.item()\n",
        "            loss_count += 1\n",
        "\n",
        "        training_accuracy = correct/total\n",
        "        training_loss = loss_total/loss_count\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        print(f\"Validation started for epoch {epoch + 1}\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i in tqdm(range(0, len(valid_data), minibatch_size)):\n",
        "                batch_data = valid_data[i:i + minibatch_size]\n",
        "                vectors, labels = process_batch(batch_data, word_embedding)\n",
        "\n",
        "                output = model(vectors)\n",
        "                predicted_labels = torch.argmax(output, dim=1)\n",
        "                correct += (predicted_labels == labels).sum().item()\n",
        "                total += len(labels)\n",
        "\n",
        "        validation_accuracy = correct/total\n",
        "\n",
        "        # Store metrics\n",
        "        training_accuracies.append(training_accuracy)\n",
        "        validation_accuracies.append(validation_accuracy)\n",
        "        training_losses.append(training_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}:\")\n",
        "        print(f\"Training accuracy: {training_accuracy:.4f}\")\n",
        "        print(f\"Validation accuracy: {validation_accuracy:.4f}\")\n",
        "        print(f\"Training loss: {training_loss:.4f}\")\n",
        "\n",
        "        # Early stopping check\n",
        "        if validation_accuracy < last_validation_accuracy and training_accuracy > last_train_accuracy:\n",
        "            stopping_condition = True\n",
        "            print(\"Training stopped to avoid overfitting!\")\n",
        "            print(f\"Best validation accuracy: {last_validation_accuracy:.4f}\")\n",
        "        else:\n",
        "            last_validation_accuracy = validation_accuracy\n",
        "            last_train_accuracy = training_accuracy\n",
        "\n",
        "        epoch += 1\n",
        "\n",
        "    # Test Phase\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(test_data), minibatch_size)):\n",
        "            batch_data = test_data[i:i + minibatch_size]\n",
        "            vectors, labels = process_batch(batch_data, word_embedding)\n",
        "\n",
        "            output = model(vectors)\n",
        "            predicted_labels = torch.argmax(output, dim=1)\n",
        "            correct += (predicted_labels == labels).sum().item()\n",
        "            total += len(labels)\n",
        "\n",
        "    test_accuracy = correct/total\n",
        "    print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    return training_accuracies, validation_accuracies, training_losses, test_accuracy\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_data, valid_data, test_data = load_data(\n",
        "        '/content/drive/MyDrive/NLPA2/training.json',\n",
        "        '/content/drive/MyDrive/NLPA2/validation.json',\n",
        "        '/content/drive/MyDrive/NLPA2/test.json'\n",
        "    )\n",
        "\n",
        "    # Load word embeddings\n",
        "    word_embedding = pickle.load(open('/content/drive/MyDrive/NLPA2/word_embedding.pkl', 'rb'))\n",
        "\n",
        "    # Create results directory\n",
        "    os.makedirs('results', exist_ok=True)\n",
        "\n",
        "    hidden_dims = [32] #[16, 32, 64, 128 ,256]\n",
        "    epochs_list = [10] #[5, 10, 30]\n",
        "    all_results = []\n",
        "\n",
        "    for hidden_dim in hidden_dims:\n",
        "        for num_epochs in epochs_list:\n",
        "            print(f\"\\n========== Training model with hidden_dim={hidden_dim}, epochs={num_epochs} ==========\")\n",
        "\n",
        "            train_accs, val_accs, train_losses, test_acc = train_and_evaluate(\n",
        "                hidden_dim, num_epochs, train_data, valid_data, test_data, word_embedding\n",
        "            )\n",
        "\n",
        "            # Save results\n",
        "            results_df = pd.DataFrame({\n",
        "                'epoch': range(1, len(train_accs) + 1),\n",
        "                'training_accuracy': train_accs,\n",
        "                'validation_accuracy': val_accs,\n",
        "                'training_loss': train_losses,\n",
        "                'test_accuracy': [test_acc] * len(train_accs),\n",
        "                'hidden_dim': [hidden_dim] * len(train_accs),\n",
        "                'max_epochs': [num_epochs] * len(train_accs)\n",
        "            })\n",
        "\n",
        "            filename = f\"results/rnn_{num_epochs}_{hidden_dim}.csv\"\n",
        "            results_df.to_csv(filename, index=False)\n",
        "\n",
        "            all_results.append({\n",
        "                'hidden_dim': hidden_dim,\n",
        "                'epochs': num_epochs,\n",
        "                'final_train_acc': train_accs[-1],\n",
        "                'final_val_acc': val_accs[-1],\n",
        "                'final_test_acc': test_acc,\n",
        "                'best_val_acc': max(val_accs),\n",
        "                'total_epochs_run': len(train_accs)\n",
        "            })\n",
        "\n",
        "    # Save combined results\n",
        "    combined_results = pd.DataFrame(all_results)\n",
        "    combined_results.to_csv('results/combined_results.csv', index=False)\n",
        "    print(\"\\nCombined results saved to results/combined_results.csv\")\n",
        "    print(\"\\nSummary of all experiments:\")\n",
        "    print(combined_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyKIdr-Fa7B_",
        "outputId": "9fe0bd36-4e2b-4568-9286-43f934eadd22"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "========== Training model with hidden_dim=32, epochs=10 ==========\n",
            "Training started for epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [01:06<00:00,  7.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation started for epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [00:03<00:00,  7.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1:\n",
            "Training accuracy: 0.2011\n",
            "Validation accuracy: 0.4012\n",
            "Training loss: 1.6361\n",
            "Training started for epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [01:05<00:00,  7.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation started for epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [00:03<00:00,  7.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2:\n",
            "Training accuracy: 0.1973\n",
            "Validation accuracy: 0.0163\n",
            "Training loss: 1.6331\n",
            "Training started for epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [01:05<00:00,  7.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation started for epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [00:03<00:00,  7.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3:\n",
            "Training accuracy: 0.2001\n",
            "Validation accuracy: 0.4012\n",
            "Training loss: 1.6323\n",
            "Training started for epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [01:04<00:00,  7.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation started for epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [00:03<00:00,  7.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4:\n",
            "Training accuracy: 0.1984\n",
            "Validation accuracy: 0.0150\n",
            "Training loss: 1.6348\n",
            "Training started for epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [01:06<00:00,  7.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation started for epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [00:03<00:00,  7.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5:\n",
            "Training accuracy: 0.1994\n",
            "Validation accuracy: 0.2087\n",
            "Training loss: 1.6280\n",
            "Training started for epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [01:05<00:00,  7.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation started for epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [00:03<00:00,  7.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6:\n",
            "Training accuracy: 0.2034\n",
            "Validation accuracy: 0.0187\n",
            "Training loss: 1.6317\n",
            "Training stopped to avoid overfitting!\n",
            "Best validation accuracy: 0.2087\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [00:02<00:00,  9.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.3875\n",
            "\n",
            "Combined results saved to results/combined_results.csv\n",
            "\n",
            "Summary of all experiments:\n",
            "   hidden_dim  epochs  final_train_acc  final_val_acc  final_test_acc  \\\n",
            "0          32      10         0.203375        0.01875          0.3875   \n",
            "\n",
            "   best_val_acc  total_epochs_run  \n",
            "0       0.40125                 6  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}